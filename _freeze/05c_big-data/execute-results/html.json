{
  "hash": "2af955afa05a5794d81a4bfd974f4a5d",
  "result": {
    "markdown": "---\ntitle: \"Data too big for memory\"\nsubtitle: \"Working efficiently with Data\"\n---\n\n\nWe've a look at approaches to speed up working with data in memory more efficiently. But what if the data we want to work with just cannot fit into memory?\n\nThere a number of approaches to deal with this situation which will depend on what exactly we need from the data to perform our operation.\n\nFor example:\n\n-   We might need to process the whole dataset but can somehow split our computation to work on smaller chunks of the data i.e. batch processing. Perhaps our data is split in many individual csv files, in which case we could write a function that works on a single file at a time and use an apply function to process all files and agreggate any result. This sort of processing is highly amenable to parallelisation.\n\n-   We might need a subset of our data which we define through filtering, selecting and other aggregating functions. In this situation, storing our data in a database and using SQL to query it for the subset of data of interest is our best option.\n\n-   A harder problem is when we indeed require all data in memory. Here our choice might require using distributed memory between many machines (e.g. on an HPC platform using MPI) as well as considering options like using single precision floats and mathematical optimisations of our algorithms.\n\n# Databases\n\nDatabases are an appropriate choice if you have large amounts of data that can't fit into memory which you only require subsets from that you can extract using queries.\n\nThere are many types of databases which are beyond the scope of this workshop. What we we will focus on here is simple relational databases that store tabular data in single flat files (a.k.a. embedded databases) as opposed to databases which are run through a server like MySQL, Microsoft SQL Server PostgresSQL or which do not store tabular data, for example MongoDB which stores data as documents.\n\nWe also focus on databases that can be queried using **SQL**. SQL (which stands for Structured Query Language) is a standardized programming language that is used to manage [relational databases](https://www.techtarget.com/searchdatamanagement/definition/relational-database) and perform various operations on the data in them.\n\nIt's good to have an idea of SQL basics when interacting with databases, but in R, many of the `dplyr` verbs are inspired by SQL commands while package `dbplyr` can take dplyr operations and translate them to SQL for querying databases as you would data.frames or tibbles.\n\nAs such we can build up our queries using a connection to a database and only collect our data explicitly when we are ready for R to execute the query.\n\n## SQLite\n\n> [SQLite](https://sqlite.org/index.html) is a C-language library that implements a [small](https://sqlite.org/footprint.html), [fast](https://sqlite.org/fasterthanfs.html), [self-contained](https://sqlite.org/selfcontained.html), [high-reliability](https://sqlite.org/hirely.html), [full-featured](https://sqlite.org/fullsql.html), SQL database engine.\n>\n> The SQLite [file format](https://sqlite.org/fileformat2.html) is stable, cross-platform, and backwards compatible.\n>\n> SQLite [source code](https://sqlite.org/src) is in the [public-domain](https://sqlite.org/copyright.html) and is free to everyone to use for any purpose.\n\nLet's start our experimentation by creating a simple SQLite database with a single table.\n\nThe data we will use is contained in `data/synthpop_10000000.parquet` and represents characteristics of 10,000,000 individuals from a synthetic population.\n\nI know this section is about data too big for memory and this is not an approach you can use for data truly larger than memory). But because I want to benchmark against in memory data and can fit it into memory on my machine, I will actually load it into memory and write it directly to the database. However, I also show a way to populate it in batches if you find the file is overloading your memory. You can also choose to use one of the smaller synthpop parquet files (e.g `data/synthpop_1000000.parquet` which contains 1,000,000 rows).\n\nSo let's load our data and have a look at it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- arrow::read_parquet(\"data/synthpop_10000000.parquet\")\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     sex age agegr                     socprof income  marital\n1   MALE  47 45-59                      FARMER   2000  MARRIED\n2 FEMALE  43 35-44 OTHER ECONOMICALLY INACTIVE     NA  MARRIED\n3   MALE  26 25-34  EMPLOYED IN PRIVATE SECTOR   1400   SINGLE\n4 FEMALE  51 45-59   EMPLOYED IN PUBLIC SECTOR     NA DIVORCED\n5 FEMALE  67   65+     LONG-TERM SICK/DISABLED    750  WIDOWED\n6   MALE  56 45-59                  UNEMPLOYED   1200  MARRIED\n                       edu sport smoke nociga alcabuse      bmi   location\n1                SECONDARY  TRUE FALSE     NA    FALSE 27.45982 Birmingham\n2 POST-SECONDARY OR HIGHER FALSE FALSE     NA    FALSE 22.30815    Grimsby\n3                SECONDARY FALSE  TRUE     15    FALSE 20.58967  Liverpool\n4 POST-SECONDARY OR HIGHER FALSE FALSE     NA    FALSE 28.93407 Colchester\n5       VOCATIONAL/GRAMMAR  TRUE  TRUE     20    FALSE 27.34375   Caerdydd\n6 POST-SECONDARY OR HIGHER  TRUE FALSE     NA    FALSE 25.71166     Welwyn\n```\n:::\n:::\n\n\nThis is quite a large dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npryr::object_size(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n600.17 MB\n```\n:::\n:::\n\n\nNext let's load some useful libraries, create our database and store the connection to said database in a variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dbConnect(drv = RSQLite::SQLite(), \"data/db.sqlite\")\n```\n:::\n\n\nThe above command creates an SQLite database at path `data/db.sqlite`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQLiteConnection>\n  Path: /Users/Anna/Documents/workflows/OHID/optimise-r/data/db.sqlite\n  Extensions: TRUE\n```\n:::\n:::\n\n\nWe can also see that the `con` object is tiny, only 2504. That's because it's just a connection to the database and does not contain any data itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npryr::object_size(con)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.50 kB\n```\n:::\n:::\n\n\nNow let's go ahead and write our data to a table in our database. For this we can use `DBI`'s function `dbWriteTable`. This will both create the table in our database and also write the data to it. The arguments we need to provide are:\n\n-   `conn` (the first argument) where we provide the connection to the database we want to write to.\n\n-   `name` the name of the table we want to to create.\n\n-   `value` the object containing the data we want to write to the table. This must be a `data.frame` or an object coercible to a `data.frame`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbWriteTable(con, name = \"synthpop\", data)\n```\n:::\n\n\nOnce writing the table is complete (this might take a little while), we can do some initial checks on our data using some more `DBI` functions:\n\n-   `dbListTables` lists the names of all tables in the database\n\n-   `dbListFields` lists all fields in a given table (`\"synthpop\"`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbListTables(con)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"synthpop\"\n```\n:::\n\n```{.r .cell-code}\ndbListFields(con, \"synthpop\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"sex\"      \"age\"      \"agegr\"    \"socprof\"  \"income\"   \"marital\" \n [7] \"edu\"      \"sport\"    \"smoke\"    \"nociga\"   \"alcabuse\" \"bmi\"     \n```\n:::\n:::\n\n\n### Chunked method to populate the database\n\nIf the data is too big to load into memory and then write to a database, an option is to populate it in chunks. This involves using `readr::read_csv_chunked` to populate a database in batches detailed in the [following blogpost](https://www.michaelc-m.com/manual_posts/2022-01-27-big-CSV-SQL.html) by Michael Culshaw-Maurer.\n\nHere's some example code of how we could populate our database from the 1,000,000 row csv file we created in the Data Input/Output section:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadr::read_csv_chunked(\"data/write/synthpop_1000000.csv\", \n                        callback = function(chunk, dummy){\n                            dbWriteTable(con, \"synthpop\", chunk, append = T)\n                        }, \n                        chunk_size = 10000)\n```\n:::\n\n\n## Interacting with database through `dplyr` & `dbplyr`\n\n`dbplyr` is the database backend for [dplyr](https://dplyr.tidyverse.org/). It allows you to use remote database tables as if they are in-memory data frames by automatically converting dplyr code into SQL.\n\nAll dplyr calls are evaluated lazily, generating SQL that is only sent to the database when you request the data.\n\nSo let's start using our connection to access some data. For that we can use function `tbl()`. Just as `dbConnect()` opens a connection to a database, we can think of `tbl()` as opening a connection to a single table in the database, in this case \\`\"synthpop\"\\`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl <- tbl(con, \"synthpop\")\n```\n:::\n\n\nIf we print `tbl` we can see all columns in the database and the first 10 rows, which looks a bit like printing a tibble, but if we look at the header of information above the data, we can see the database source as well as `[?? x 12]` in the dimensions summary. That's because `tbl` does not contain the full table data, just a connection to it, and therefore is not aware of the number of rows of the complete table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   table<synthpop> [?? x 12]\n# Database: sqlite 3.40.0 [/Users/Anna/Documents/workflows/OHID/optimise-r/data/db.sqlite]\n   sex      age agegr socprof    income marital edu   sport smoke nociga alcab…¹\n   <chr>  <dbl> <chr> <chr>       <dbl> <chr>   <chr> <int> <int>  <dbl>   <int>\n 1 MALE      47 45-59 FARMER       2000 MARRIED SECO…     1     0     NA       0\n 2 FEMALE    43 35-44 OTHER ECO…     NA MARRIED POST…     0     0     NA       0\n 3 MALE      26 25-34 EMPLOYED …   1400 SINGLE  SECO…     0     1     15       0\n 4 FEMALE    51 45-59 EMPLOYED …     NA DIVORC… POST…     0     0     NA       0\n 5 FEMALE    67 65+   LONG-TERM…    750 WIDOWED VOCA…     1     1     20       0\n 6 MALE      56 45-59 UNEMPLOYED   1200 MARRIED POST…     1     0     NA       0\n 7 MALE      86 65+   RETIRED      1260 MARRIED SECO…     1     0     NA       0\n 8 MALE      59 45-59 LONG-TERM…   1400 MARRIED POST…     1     0     NA       0\n 9 FEMALE    52 45-59 EMPLOYED …   1500 MARRIED SECO…     1     1     30       1\n10 FEMALE    22 16-24 PUPIL OR …    600 SINGLE  SECO…     0     0     NA       0\n# … with more rows, 1 more variable: bmi <dbl>, and abbreviated variable name\n#   ¹​alcabuse\n```\n:::\n:::\n\n\nLet's have a look at the `tbl` class. The important class identifiers are `\"tbl_dbi\"` and `\"tbl_sql\"` which indicate any data manipulation on the tbl will be translated to SQL, will be lazy and will return another `\"tbl_dbi\"`, not the actual result of the query.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"tbl_SQLiteConnection\" \"tbl_dbi\"              \"tbl_sql\"             \n[4] \"tbl_lazy\"             \"tbl\"                 \n```\n:::\n:::\n\n\n### Getting the number of rows of a database table\n\nSo what if we did want to know the number of rows of the `\"synthpop\"` table to double check we have written in fully?\n\nWe might try a familiar R function, `nrow()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA\n```\n:::\n:::\n\n\nBut this doesn't work! That's because there is no translation of R function `nrow()` to SQL.\n\nWe'll need to frame our request as something that can be translated into an SQL query by `dbplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl %>% \n    summarize(n = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [1 x 1]\n# Database: sqlite 3.40.0 [/Users/Anna/Documents/workflows/OHID/optimise-r/data/db.sqlite]\n         n\n     <int>\n1 10000000\n```\n:::\n:::\n\n\n`summarise(n())` get's translated to an SQL `COUNT` function, which is an [SQL aggregate function](https://www.sqlite.org/lang_aggfunc.html) that returns one value, hence what is returned to us is another `tbl_dbi` of 1 x 1 dimensions.\n\n::: callout-note\nTo learn more about which R functions are translated by dbplyr to SQL have a look at the package's vignettes on [Verb](https://dbplyr.tidyverse.org/articles/translation-verb.html) and [Function](https://dbplyr.tidyverse.org/articles/translation-function.html) translation.\n:::\n\nWe can inspect the SQL statement generated by `dbplyr` by piping the query to `dplyr`'s `show_query()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl %>% \n    summarize(n = n()) %>%\n    show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT COUNT(*) AS `n`\nFROM `synthpop`\n```\n:::\n:::\n\n\nRemember that just running the query returns another `tbl_dbi`. To be able to compute on it in R need to collect it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndb_nrows <- tbl %>% \n    summarize(n = n()) %>%\n    collect()\n\ndb_nrows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n         n\n     <int>\n1 10000000\n```\n:::\n\n```{.r .cell-code}\npull(db_nrows) == nrow(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nWe have now checked that our data was fully written to our database table.\n\n### Filtering, selecting and summarising\n\nAs mentioned, many of `dplyr` verbs as well as number of aggregating and arithmetic functions can be translated to SQL by `dbplyr`. For greatest it's good to try and perform as many operations in SQL before collecting the data. These are performed by the databases SQL engine which is generally more efficient when working with large data.\n\nLet's try a few examples.\n\nLet's put together a query that filter values in a few columns and then selects a few columns to return:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(tbl,\n    age > 30,\n    sex == \"MALE\",\n    sport == TRUE\n) %>%\n    select(income, age, marital)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.40.0 [/Users/Anna/Documents/workflows/OHID/optimise-r/data/db.sqlite]\n   income   age marital\n    <dbl> <dbl> <chr>  \n 1   2000    47 MARRIED\n 2   1200    56 MARRIED\n 3   1260    86 MARRIED\n 4   1400    59 MARRIED\n 5   1100    78 MARRIED\n 6   1500    34 MARRIED\n 7     NA    44 SINGLE \n 8   1500    50 MARRIED\n 9     NA    46 MARRIED\n10     NA    56 MARRIED\n# … with more rows\n```\n:::\n:::\n\n\nAgain, running the query without collecting does not return the full query result but can help check what your query is doing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(tbl,\n       age > 30,\n       sex == \"MALE\",\n       sport == TRUE) %>%\n    select(income, age, marital) %>%\n    show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT `income`, `age`, `marital`\nFROM `synthpop`\nWHERE (`age` > 30.0) AND (`sex` = 'MALE') AND (`sport` = 1)\n```\n:::\n:::\n\n\nAnd adding show_query() to the end of the pipe shows the SQL translation of the query.\n\n#### Query 1\n\nLet's try another one:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(tbl,\n       age > 50L & age < 60L, \n       income < 300) %>%\n    arrange(bmi, age, income, nociga, sex) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:     SQL [?? x 12]\n# Database:   sqlite 3.40.0 [/Users/Anna/Documents/workflows/OHID/optimise-r/data/db.sqlite]\n# Ordered by: bmi, age, income, nociga, sex\n   sex     age agegr socprof     income marital edu   sport smoke nociga alcab…¹\n   <chr> <dbl> <chr> <chr>        <dbl> <chr>   <chr> <int> <int>  <dbl>   <int>\n 1 MALE     51 45-59 OTHER ECON…    200 MARRIED VOCA…     1     0     NA       0\n 2 MALE     51 45-59 OTHER ECON…    200 MARRIED VOCA…     1     0     NA       0\n 3 MALE     51 45-59 OTHER ECON…    200 DIVORC… VOCA…     1     0     NA       0\n 4 MALE     51 45-59 OTHER ECON…    200 MARRIED VOCA…     0     0     NA       0\n 5 MALE     51 45-59 OTHER ECON…    200 LEGALL… VOCA…     1     1     10       0\n 6 MALE     51 45-59 OTHER ECON…    200 SINGLE  PRIM…     1     1     10       0\n 7 MALE     51 45-59 OTHER ECON…    200 LEGALL… VOCA…     1     1     10       1\n 8 MALE     51 45-59 OTHER ECON…    200 LEGALL… VOCA…     1     1     20       1\n 9 MALE     51 45-59 OTHER ECON…    200 DIVORC… VOCA…     1     1     20       1\n10 MALE     51 45-59 OTHER ECON…    200 LEGALL… PRIM…     1     1     20       1\n# … with more rows, 1 more variable: bmi <dbl>, and abbreviated variable name\n#   ¹​alcabuse\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(tbl,\n       age > 50L & age < 60L, \n       income < 300) %>%\n    arrange(bmi, age, income, nociga, sex) %>%\n    show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n```\n:::\n:::\n\n\nLet's wrap this query in a function we can use to benchmark how long it takes to execute.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1 <- function(tbl) {\n    filter(tbl,\n       age > 50L & age < 60L, \n       income < 300) %>%\n    arrange(bmi, age, income, nociga, sex)\n}\n```\n:::\n\n\n#### Query 2\n\nLet's put together one more example to use for benchmarking which includes some aggregating and arithmetic functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(tbl,\n       age > 65L,\n       sex == \"MALE\",\n       sport == TRUE,\n       !is.na(income),\n       !is.na(marital)) %>%\n    group_by(marital) %>%\n    summarise(min_income = min(income),\n              max_income = max(income),\n              mean_income = mean(income))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [6 x 4]\n# Database: sqlite 3.40.0 [/Users/Anna/Documents/workflows/OHID/optimise-r/data/db.sqlite]\n  marital            min_income max_income mean_income\n  <chr>                   <dbl>      <dbl>       <dbl>\n1 DE FACTO SEPARATED        300       7000       1227.\n2 DIVORCED                  300      10000       1403.\n3 LEGALLY SEPARATED         300       1350        969.\n4 MARRIED                   158      10000       1323.\n5 SINGLE                    300      10000       1334.\n6 WIDOWED                   158      10000       1485.\n```\n:::\n:::\n\n\nLet's look at the SQL translation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(tbl,\n       age > 65L,\n       sex == \"MALE\",\n       sport == TRUE,\n       !is.na(income),\n       !is.na(marital)) %>%\n    group_by(marital) %>%\n    summarise(min_income = min(income),\n              max_income = max(income),\n              mean_income = mean(income)) %>%\n    show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT\n  `marital`,\n  MIN(`income`) AS `min_income`,\n  MAX(`income`) AS `max_income`,\n  AVG(`income`) AS `mean_income`\nFROM `synthpop`\nWHERE\n  (`age` > 65) AND\n  (`sex` = 'MALE') AND\n  (`sport` = 1) AND\n  (NOT((`income` IS NULL))) AND\n  (NOT((`marital` IS NULL)))\nGROUP BY `marital`\n```\n:::\n:::\n\n\nAnd again, wrap it in a function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_2 <- function(tbl) {\n    filter(tbl,\n    age > 65L,\n       sex == \"MALE\",\n       sport == TRUE,\n       !is.na(income),\n       !is.na(marital)) %>%\n    group_by(marital) %>%\n    summarise(min_income = min(income),\n              max_income = max(income),\n              mean_income = mean(income)) %>%\n        arrange(marital)\n}\n```\n:::\n\n\nOK, let's now run some starting benchmarks against running the same query on the data in memory:\n\n##### Query 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    df = query_1(data),\n    sqlite = query_1(tbl) %>%\n        collect(),\n    check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df            121ms    121ms      8.30  422.77MB     33.2\n2 sqlite        606ms    606ms      1.65    3.49MB      0  \n```\n:::\n:::\n\n\n*Note I've turned off checking for this benchmark because of the difference in how `dplyr` handles `NA`s when arranging data in data.frames (`NA`'s at the end) vs how SQLite's engine does (`NA`'s at the top).*\n\n##### Query 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    df = query_2(data),\n    sqlite = query_2(tbl) %>%\n        collect()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df            787ms    787ms      1.27     435MB    10.2 \n2 sqlite        681ms    681ms      1.47     567KB     1.47\n```\n:::\n:::\n\n\nIn this first test of performance, databases come out slower. That shouldn't surprise us though. Working with in memory data (that still allows for the memory required for computation) will always be faster because there is no **I/O** cost to the query (once it has been loaded into memory), whereas executing and collecting the query from the database involves returning the data from disk. We can see though that working with a database is much more memory efficient, which given the topic of the chapter is working with data that does not fit into memory, shows it is a good approach for this use case.\n\n### Indexing\n\nIndexes are a way to improve the performance of your read queries, particularly ones with filters (`WHERE`) on them. They're data structures that exist in your database engine, outside of whatever table they work with, that point to data you're trying to query.\n\nThey work similar to how indexes in the back of a book do. They contain the ordered values of the column you create them on along with information about the location of the rows containing each value in the original table.\n\nSo just like you might use an index to find a recipe instead of flicking through an entire recipe book, database indexes allow you to look up the values in columns and the location of the rows containing them in your original table without scanning the full table. A well crafted index can produce impressive query speed ups!\n\nThis does come at a cost. They take up space within your database, increasing it's overall size, and they also slow down updating any tables containing indexes as the indexes must also be updated. Crafting indexes is also a bit of an art, as creating an index that speeds up a given query might actually slow another one down!\n\nThe details of good indexing strategy are a big topic that is well beyond the scope of this workshop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.copy(\n    from = \"data/db.sqlite\",\n    to = \"data/db_idx.sqlite\"\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nLet's connect to the database we're going to index as well as create a connection to the `\"synthpop\"` table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon_idx <- dbConnect(RSQLite::SQLite(), \"data/db_idx.sqlite\")\n\ntbl_idx <- tbl(con_idx, \"synthpop\")\n```\n:::\n\n\nNow **let's create our first index to try and improve the performance of our select (`WHERE`) operation in query 1.**\n\n#### Query 1\n\nLet's remind ourself what the query is actually doing. This time we'll use another `dplyr` function, `explain()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(tbl_idx) %>%\n  explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n\n<PLAN>\n  id parent notused                       detail\n1  3      0       0                SCAN synthpop\n2 33      0       0 USE TEMP B-TREE FOR ORDER BY\n```\n:::\n:::\n\n\n`explain()` translates to the `EXPLAIN QUERY PLAN` command in SQLite databases. It includes the `SQL` translation of our query but is used primarily to obtain a high-level description of the strategy or plan that SQLite uses to implement a specific SQL query. Most significantly, **`EXPLAIN QUERY PLAN` reports on the way in which the query uses database indices**. The relevant information is found in the `detail` column of the bottom table of the output.\n\nThe output of piping query 1 into `explain()` indicates that the SQLite engine is using a full scan of the `\"synthpop\"` table to locate the rows matching our select (`WHERE`) condition. It then uses a temporary Sorting B-Tree for ordering the data. When you use `ORDER BY` without an index on the column to be sorted, SQLite builds up a temporary data structure that contains the sorted results each time the query is executed. That data structure will use up space in memory (or on disk, depending on the circumstances) and will be torn down after the query is executed.\n\n::: callout-tip\nTo find out more about the SQLite `EXPLAIN QUERY PLAN` command, head to [SQLite doumentation](https://www.sqlite.org/eqp.html).\n:::\n\nTime to create an index. To do so we use function `dbExecute()` on our database connection and pass it a character string of the SQL statement to create an index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con_idx,\n          \"CREATE INDEX synthpop_age_inc_idx ON synthpop (age, income);\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nLet's break down the statement:\n\n-   `CREATE INDEX` is the command to create an index.\n\n-   `synthpop_age_inc_idx` is the name of the index we want to create. It's good practice to include an indication of the table as well as the columns used to create the index in name of the index.\n\n-   `ON synthpop` indicates that the index is being created on table `synthpop`.\n\n-   `(age, income)` the parenthesis indicates the columns we want to include in our index. Indexes can be created using one or multiple columns. Here, because our filter statement includes seraching for values on both `age` and `income`, we include both columns for better performance. Note however that this inevitably takes up more disk space and more time to create (and in future update) the index.\n\nOK, let's check the query plan to see if SQLite plans to use our index:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n\n<PLAN>\n  id parent notused\n1  4      0       0\n2 32      0       0\n                                                              detail\n1 SEARCH synthpop USING INDEX synthpop_age_inc_idx (age>? AND age<?)\n2                                       USE TEMP B-TREE FOR ORDER BY\n```\n:::\n:::\n\n\nIndeed! The query is not `SCAN`ning the full table anymore but is instead using our index to `SEARCH` for values in the index matching our filter statement.\n\nLet's see whether we've improved our query's performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n   no_index = query_1(tbl) %>%\n    collect(),\n   index = query_1(tbl_idx) %>%\n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 no_index      617ms    617ms      1.62    3.44MB        0\n2 index         126ms    126ms      7.89    3.44MB        0\n```\n:::\n:::\n\n\nIndeed we have, roughly a 5x speed up. Not bad! But we could do better!\n\nBecause indexes are sorted data structures and their benefit comes from how binary search works, it's important to ensure that our indexed columns have what is called \"high cardinality\". All this means is that the indexed data has a lot of uniqueness.\n\nWhile our `age` column has 79 unique values, our income column has 406, i.e. income has higher cardinality than income.\n\nA multi-column index will initially use the first column to search, then the second an so on. So instead of putting `age` at the front of our index, let's drop our first index using the `DROP INDEX` command and let's instead create a new index with income first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con_idx,\n          \"DROP INDEX synthpop_age_inc_idx;\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\ndbExecute(con_idx,\n          \"CREATE INDEX synthpop_inc_age_idx ON synthpop (income, age);\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nLet's inspect our query plan which reveals that, indeed, our index now searches through income first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n\n<PLAN>\n  id parent notused                                                      detail\n1  4      0       0 SEARCH synthpop USING INDEX synthpop_inc_age_idx (income<?)\n2 35      0       0                                USE TEMP B-TREE FOR ORDER BY\n```\n:::\n:::\n\n\nLet's run our benchmarks again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n   no_index = query_1(tbl) %>%\n    collect(),\n   index = query_1(tbl_idx) %>%\n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 no_index    622.9ms  622.9ms      1.61    3.44MB     0   \n2 index        66.9ms   68.8ms     14.4     3.44MB     2.06\n```\n:::\n:::\n\n\nMuch better! We're now approaching a 10x speed up!\n\nSo, do you think we can speed up the query even more? What about the arrange part of the query?\n\nYou may have noticed that the `ORDER BY` part of the query is still using a temporary B-TREE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n\n<PLAN>\n  id parent notused                                                      detail\n1  4      0       0 SEARCH synthpop USING INDEX synthpop_inc_age_idx (income<?)\n2 35      0       0                                USE TEMP B-TREE FOR ORDER BY\n```\n:::\n:::\n\n\nAn index can be used to speed up sorting only if the query allows to return the rows in the order in which they are stored in the index. **Because our index does not include many of the columns we are using in the sort operation, and most importantly, the first one (`bmi`) the index is ignored by `ORDER BY`.**\n\nWe might consider creating another index to take care of the `ORDER BY` operation and include all the columns involved in the order that we want them sorted.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con_idx,\n          \"CREATE INDEX synthpop_arrange1_idx ON synthpop (bmi, age, income, nociga, sex);\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nLet's see if that improves performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n   no_index = query_1(tbl) %>%\n    collect(),\n   index = query_1(tbl_idx) %>%\n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 no_index      611ms    611ms      1.64    3.44MB     0   \n2 index         486ms    486ms      2.06    3.44MB     2.06\n```\n:::\n:::\n\n\nOh dear! The query is now much slower and not a huge improvement to our non-indexed database! What's going on?\n\nLet's inspect our query plan:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n\n<PLAN>\n  id parent notused                                          detail\n1  4      0       0 SCAN synthpop USING INDEX synthpop_arrange1_idx\n```\n:::\n:::\n\n\nNow what we see is that the engine is indeed using our `synthpop_arrange1_idx` index but is only using that one. Not only that, it is now performing a full `SCAN` of the arrange index table.\n\nAn important thing to note is that, in SQLite, **each table in the FROM clause of a query can use at most one index and SQLite strives to use at least one index on each table**. So it cannot use one index for the `WHERE` part of the query and another for the `ORDER BY` part.\n\nIn this case, the engine determines that the least costly query plan is to just use the `synthpop_arrange1_idx` index because all the information it needs is stored within and therefore does not require a lookup in the original `synthpop` table to retrieve further data. It knows the data is stored in the correct order but to perform the `WHERE` operation, it does need to scan the full index table.\n\nBut why does this in practice end up slower? That's because the `WHERE` operation actually returns a much smaller subset of the data. So optimising that part of the query and the using a B-TREE for sorting actually ends up much faster in practice. However, the query optimiser has no way of knowing this upfront (and may not be the case if the `WHERE` operation returns a much bigger subset), so concludes that (wrongly in our case) that using the `synthpop_arrange1_idx` index is most efficient.\n\nSo at least for this query, let's consider the `synthpop_arrange1_idx` index an drop it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con_idx,\n          \"DROP INDEX synthpop_arrange1_idx;\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nquery_1(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM `synthpop`\nWHERE (`age` > 50 AND `age` < 60) AND (`income` < 300.0)\nORDER BY `bmi`, `age`, `income`, `nociga`, `sex`\n\n<PLAN>\n  id parent notused                                                      detail\n1  4      0       0 SEARCH synthpop USING INDEX synthpop_inc_age_idx (income<?)\n2 35      0       0                                USE TEMP B-TREE FOR ORDER BY\n```\n:::\n:::\n\n\nNow the optimiser goes back to using the `synthpop_inc_age_idx` index.\n\n#### Query 2\n\nSo we've made Query 1 faster but what about query 2?\n\nLet's check whether it also helps with query 2:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n   no_index = query_2(tbl) %>%\n    collect(),\n   index = query_2(tbl_idx) %>%\n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 no_index      752ms    752ms      1.33     528KB     1.33\n2 index         714ms    714ms      1.40     525KB     1.40\n```\n:::\n:::\n\n\nWell that's not good! The index seems to have made query 2 a slower?! If we use `explain()` to dig into it we see it's still doing a full scan but now the optimiser has to also evaluate a potential query plan that might involve our `synthpop_inc_age_idx` index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_2(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT\n  `marital`,\n  MIN(`income`) AS `min_income`,\n  MAX(`income`) AS `max_income`,\n  AVG(`income`) AS `mean_income`\nFROM `synthpop`\nWHERE\n  (`age` > 65) AND\n  (`sex` = 'MALE') AND\n  (`sport` = 1) AND\n  (NOT((`income` IS NULL))) AND\n  (NOT((`marital` IS NULL)))\nGROUP BY `marital`\nORDER BY `marital`\n\n<PLAN>\n  id parent notused                       detail\n1  7      0       0                SCAN synthpop\n2 21      0       0 USE TEMP B-TREE FOR GROUP BY\n```\n:::\n:::\n\n\nLet's create an index to improve the performance of query 2. Let's focus again on the `WHERE` part of the query.\n\nWe might start by creating an index using all columns involved in the order of decreasing cardinality:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con_idx,\n          \"CREATE INDEX synthpop_inc_age_mar_sex_sp_idx ON synthpop (income, age, marital, sex, sport);\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nLet's check our query plan and benchmark:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_2(tbl_idx) %>%\n    explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT\n  `marital`,\n  MIN(`income`) AS `min_income`,\n  MAX(`income`) AS `max_income`,\n  AVG(`income`) AS `mean_income`\nFROM `synthpop`\nWHERE\n  (`age` > 65) AND\n  (`sex` = 'MALE') AND\n  (`sport` = 1) AND\n  (NOT((`income` IS NULL))) AND\n  (NOT((`marital` IS NULL)))\nGROUP BY `marital`\nORDER BY `marital`\n\n<PLAN>\n  id parent notused\n1  7      0       0\n2 21      0       0\n                                                              detail\n1 SCAN synthpop USING COVERING INDEX synthpop_inc_age_mar_sex_sp_idx\n2                                       USE TEMP B-TREE FOR GROUP BY\n```\n:::\n\n```{.r .cell-code}\nbench::mark(\n   no_index = query_2(tbl) %>%\n    collect(),\n   index = query_2(tbl_idx) %>%\n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 no_index      672ms    672ms      1.49     524KB     1.49\n2 index         526ms    526ms      1.90     524KB     1.90\n```\n:::\n:::\n\n\nWe see a small improvement. At least the query is not slower now!\n\n::: callout-important\n#### Indexing Take Aways:\n\nIndexes can be a useful strategy for improving specific query perfomance. HOWEVER:\n\n-   We have only scraped the surface of the types of indexes available as well as how to determine when and how to deploy them.\n\n-   They are fiddly to create and can have unexpected effects on different queries.\n\n-   They take time to create and update and take up space of disk (our indexed database is now 1.2402647\\times 10^{9} compared to 8.2244403\\times 10^{8} of our original database!\n\n-   Trying to create new indexes to optimise each new query quickly get out of hand and required a lot of knowledge/experimentation.\n:::\n\nBut! The next section provides some useful perspective!\n\n### DuckDB\n\nWhile SQLite is ubiquitous in the world of embedded databases, and it supports complex analytical queries, **SQLite is primarily designed for fast online transaction processing (OLTP)**, employing row-oriented execution.\n\nThere is however a rather recent type of embedded (flat) database called [**DuckDB**](https://duckdb.org/).\n\nDuckDB can be far more efficient for complex analytics queries on large amount of data from a database, more common in analytics workflow.\n\nFrom the [DuckDB website](https://duckdb.org/why_duckdb):\n\n> DuckDB is designed to support **analytical query workloads**, also known as **Online analytical processing (OLAP).**\n>\n> These workloads are characterized by complex, relatively long-running queries that process significant portions of the stored dataset, for example aggregations over entire tables or joins between several large tables.\n>\n> Changes to the data are expected to be rather large-scale as well, with several rows being appended, or large portions of tables being changed or added at the same time.\n>\n> DuckDB contains a [**columnar-vectorized query execution engine**]{.underline}, where queries are still interpreted, but a large batch of values (a \"vector\") are processed in one operation. This greatly reduces overhead present in traditional systems such as PostgreSQL, MySQL or SQLite which process each row sequentially\n\nIt also has a nice API to R handled through package `duckdb`. I highly recommend checking the DuckDB documentation to learn more about it's features, but in general, you can interact with DuckDB databases in R as you would any other database.\n\nSo let's create a DuckDB database with the same data and benchmark our queries against it.\n\nAgain we can use `dbConnect()` to both create a database using a `duckdb::duckdb()` driver and open a connection to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon_duckdb <- dbConnect(duckdb::duckdb(), \"data/db.duckdb\")\ncon_duckdb\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<duckdb_connection fadd0 driver=<duckdb_driver 923e0 dbdir='data/db.duckdb' read_only=FALSE bigint=numeric>>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndbWriteTable(con_duckdb, \"synthpop\", data)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndbListTables(con_duckdb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"synthpop\"\n```\n:::\n\n```{.r .cell-code}\ndbListFields(con_duckdb, \"synthpop\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"sex\"      \"age\"      \"agegr\"    \"socprof\"  \"income\"   \"marital\" \n [7] \"edu\"      \"sport\"    \"smoke\"    \"nociga\"   \"alcabuse\" \"bmi\"     \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_duckdb <- tbl(con_duckdb, \"synthpop\")\ntbl_duckdb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   table<synthpop> [?? x 12]\n# Database: DuckDB 0.6.2-dev1166 [root@Darwin 21.4.0:R 4.2.1/data/db.duckdb]\n   sex      age agegr socprof    income marital edu   sport smoke nociga alcab…¹\n   <chr>  <dbl> <chr> <chr>       <dbl> <chr>   <chr> <lgl> <lgl>  <dbl> <lgl>  \n 1 MALE      47 45-59 FARMER       2000 MARRIED SECO… TRUE  FALSE     NA FALSE  \n 2 FEMALE    43 35-44 OTHER ECO…     NA MARRIED POST… FALSE FALSE     NA FALSE  \n 3 MALE      26 25-34 EMPLOYED …   1400 SINGLE  SECO… FALSE TRUE      15 FALSE  \n 4 FEMALE    51 45-59 EMPLOYED …     NA DIVORC… POST… FALSE FALSE     NA FALSE  \n 5 FEMALE    67 65+   LONG-TERM…    750 WIDOWED VOCA… TRUE  TRUE      20 FALSE  \n 6 MALE      56 45-59 UNEMPLOYED   1200 MARRIED POST… TRUE  FALSE     NA FALSE  \n 7 MALE      86 65+   RETIRED      1260 MARRIED SECO… TRUE  FALSE     NA FALSE  \n 8 MALE      59 45-59 LONG-TERM…   1400 MARRIED POST… TRUE  FALSE     NA FALSE  \n 9 FEMALE    52 45-59 EMPLOYED …   1500 MARRIED SECO… TRUE  TRUE      30 TRUE   \n10 FEMALE    22 16-24 PUPIL OR …    600 SINGLE  SECO… FALSE FALSE     NA FALSE  \n# … with more rows, 1 more variable: bmi <dbl>, and abbreviated variable name\n#   ¹​alcabuse\n```\n:::\n:::\n\n\n## Benchmark Queries\n\nNow let's go ahead and run our queries again, this time including running them on the `duckdb` database we just created.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    df = query_1(data),\n    sqlite = query_1(tbl) %>%\n        collect(),\n    sqlite_idx = query_1(tbl_idx) %>%\n        collect(),\n    duckdb = query_1(tbl_duckdb) %>%\n        collect(),\n    check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df            117ms  119.7ms      7.99  269.51MB     2.66\n2 sqlite        635ms    635ms      1.57    3.45MB     0   \n3 sqlite_idx     65ms   65.4ms     15.3     3.44MB     2.18\n4 duckdb         89ms   89.8ms     11.1     1.65MB     2.22\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    df = query_2(data),\n    sqlite = query_2(tbl) %>%\n        collect(),\n    sqlite_idx = query_2(tbl_idx) %>%\n        collect(),\n    duckdb = query_2(tbl_duckdb) %>%\n        collect()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df          919.4ms  919.4ms      1.09     435MB     9.79\n2 sqlite      663.9ms  663.9ms      1.51     529KB     0   \n3 sqlite_idx  541.4ms  541.4ms      1.85     527KB     1.85\n4 duckdb       94.8ms   96.4ms     10.4      861KB    10.4 \n```\n:::\n:::\n\n\nWow! DuckDB is much faster than SQLite, can compete with and beat an indexed SQL database and can be even faster than running the queries on in-memory data!! And still very memory efficient. And all this without even having to think about indexes!! 🤩 🎉\n\nThis is definitely a database type you should know about!\n\n## Accessing data through the `arrow` package\n\n**Arrow** is software development platform for building high performance applications. As we've seen already, The `arrow` R package provides functionality for fast reading and writing of flat files as well as more efficient binary file formats.\n\nIt **also provides functions for opening connections to files as well as directories of files**, much like we did with databases, and because it has deep integration with `dplyr`, it allows us to perform queries on out of memory data as we've been doing with our databases.\n\n### Accessing single files as arrow tables\n\nWe can read in a single large csv, arrow or parquet file using the appropriate `arrow` function but instead of reading it in as a data.frame, we can use `as_data_frame = FALSE` to open it as an arrow table. Because of how Arrow allocates memory, arrow tables are much more memory efficient representations of tabular data that could mean data that won't fit into memory as an\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrow_tbl <- arrow::read_parquet(\"data/synthpop_10000000.parquet\", \n                      as_data_frame = FALSE)\n\n\narrow_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n10000000 rows x 13 columns\n$sex <string>\n$age <int32>\n$agegr <string>\n$socprof <string>\n$income <int32>\n$marital <string>\n$edu <string>\n$sport <bool>\n$smoke <bool>\n$nociga <int32>\n$alcabuse <bool>\n$bmi <double>\n$location <string>\n```\n:::\n:::\n\n\nMany dplyr verbs can be used to interrogate this arrow table. To demonstrated let's execute query 1 on our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(arrow_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable (query)\nsex: string\nage: int32\nagegr: string\nsocprof: string\nincome: int32\nmarital: string\nedu: string\nsport: bool\nsmoke: bool\nnociga: int32\nalcabuse: bool\nbmi: double\nlocation: string\n\n* Filter: (((age > 50) and (age < 60)) and (income < 300))\n* Sorted by bmi [asc], age [asc], income [asc], nociga [asc], sex [asc]\nSee $.data for the source Arrow object\n```\n:::\n:::\n\n\nJust like with databases, the query does not return a tibble. We again need to `collect()` the results of our query for it be converted to a tibble:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_1(arrow_tbl) %>%\n    collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15,427 × 13\n   sex      age agegr socprof income marital edu      sport smoke nociga alcab…¹\n   <chr>  <int> <chr> <chr>    <int> <chr>   <chr>    <lgl> <lgl>  <int> <lgl>  \n 1 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE       5 FALSE  \n 2 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE       8 FALSE  \n 3 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE       8 FALSE  \n 4 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE       8 FALSE  \n 5 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE       8 FALSE  \n 6 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE       8 FALSE  \n 7 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE      10 FALSE  \n 8 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE      10 FALSE  \n 9 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE      10 FALSE  \n10 FEMALE    51 45-59 FARMER     250 MARRIED VOCATIO… TRUE  TRUE      10 FALSE  \n# … with 15,417 more rows, 2 more variables: bmi <dbl>, location <chr>, and\n#   abbreviated variable name ¹​alcabuse\n```\n:::\n:::\n\n\nGiven that the `arrow_tbl` is actually in memory, we can compare query execution time to the in memory `data`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(in_mem_csv = query_1(data),\n            arrow_tbl = query_1(arrow_tbl) %>%\n    collect(),\n    check = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 in_mem_csv    117ms    117ms      8.54     270MB    25.6 \n2 arrow_tbl    33.8ms   34.7ms     27.9      282KB     2.15\n```\n:::\n:::\n\n\nWOw! that's must faster than performing the query even on an in memory data.frame. Impressive!\n\n### Accessing data as arrow datasets\n\nAnother way to access files through R is by opening them as a dataset with function `arrow::open_dataset()`.\n\nWe can open a single file or a whole directory of files, formatted in any of the formats arrow can handle.\n\nThis does not load the data into memory. Instead `open_dataset()` scans the content of the file(s) to identify the name of the columns and their data types.\n\n#### Accessing single files as arrow datasets\n\nLet's open a single file as a dataset first. To do so we supply the path to the files as well as the format it's stored in.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrow_dt_csv <- arrow::open_dataset(\"data/synthpop_10000000.parquet\", format = \"parquet\")\n\n\nbench::mark(\n    df = query_1(data),\n    sqlite = query_1(tbl) %>%\n        collect(),\n    duckdb = query_1(tbl_duckdb) %>%\n        collect(),\n    arrow_dt_csv = query_1(arrow_dt_csv) %>%\n    collect(),\n    check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  expression        min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>   <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df            117.8ms    121ms      8.30  269.51MB     8.30\n2 sqlite        611.8ms    612ms      1.63    3.44MB     0   \n3 duckdb         87.2ms     88ms     11.4     1.58MB     2.27\n4 arrow_dt_csv  329.3ms    346ms      2.89  279.69KB     0   \n```\n:::\n:::\n\n\n#### Accessing directories as arrow datasets\n\nWe can also use the same function to open a directory of files stored in the same format. This might be appropriate when your data generation involves creating data in batches that end up in separate files and for some reason you don't want to be writing them to a database.\n\nThe directory structure can help improve performance of queries too depending on how data is partitioned across directories. In some ways you can think of the physical partitioning as a physical index that can be used in a query to completely skip certain files.\n\nLet's have a look at what this means by actually creating such a directory structure from our dataset.\n\nFirst let's create a directory to partition it into. Then we can use function `arrow::write_dataset()` to write out our data partitioned according to any variables we specify in the `partitioning` argument. Here we choose to partition across age. Let's also write data out in efficient parquet files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrow::write_dataset(data, \n                     path = \"data/arrow_dataset\",\n                     format = \"parquet\",\n                     partitioning = \"age\")\n```\n:::\n\n\nLet's use `fs::dir_tree()` to see the structure of the `arrow_dat` directory we just created:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::dir_tree(\"data/arrow_dataset/\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndata/arrow_dataset/\n├── age=16\n│   └── part-0.parquet\n├── age=17\n│   └── part-0.parquet\n├── age=18\n│   └── part-0.parquet\n├── age=19\n│   └── part-0.parquet\n├── age=20\n│   └── part-0.parquet\n├── age=21\n│   └── part-0.parquet\n├── age=22\n│   └── part-0.parquet\n├── age=23\n│   └── part-0.parquet\n├── age=24\n│   └── part-0.parquet\n├── age=25\n│   └── part-0.parquet\n├── age=26\n│   └── part-0.parquet\n├── age=27\n│   └── part-0.parquet\n├── age=28\n│   └── part-0.parquet\n├── age=29\n│   └── part-0.parquet\n├── age=30\n│   └── part-0.parquet\n├── age=31\n│   └── part-0.parquet\n├── age=32\n│   └── part-0.parquet\n├── age=33\n│   └── part-0.parquet\n├── age=34\n│   └── part-0.parquet\n├── age=35\n│   └── part-0.parquet\n├── age=36\n│   └── part-0.parquet\n├── age=37\n│   └── part-0.parquet\n├── age=38\n│   └── part-0.parquet\n├── age=39\n│   └── part-0.parquet\n├── age=40\n│   └── part-0.parquet\n├── age=41\n│   └── part-0.parquet\n├── age=42\n│   └── part-0.parquet\n├── age=43\n│   └── part-0.parquet\n├── age=44\n│   └── part-0.parquet\n├── age=45\n│   └── part-0.parquet\n├── age=46\n│   └── part-0.parquet\n├── age=47\n│   └── part-0.parquet\n├── age=48\n│   └── part-0.parquet\n├── age=49\n│   └── part-0.parquet\n├── age=50\n│   └── part-0.parquet\n├── age=51\n│   └── part-0.parquet\n├── age=52\n│   └── part-0.parquet\n├── age=53\n│   └── part-0.parquet\n├── age=54\n│   └── part-0.parquet\n├── age=55\n│   └── part-0.parquet\n├── age=56\n│   └── part-0.parquet\n├── age=57\n│   └── part-0.parquet\n├── age=58\n│   └── part-0.parquet\n├── age=59\n│   └── part-0.parquet\n├── age=60\n│   └── part-0.parquet\n├── age=61\n│   └── part-0.parquet\n├── age=62\n│   └── part-0.parquet\n├── age=63\n│   └── part-0.parquet\n├── age=64\n│   └── part-0.parquet\n├── age=65\n│   └── part-0.parquet\n├── age=66\n│   └── part-0.parquet\n├── age=67\n│   └── part-0.parquet\n├── age=68\n│   └── part-0.parquet\n├── age=69\n│   └── part-0.parquet\n├── age=70\n│   └── part-0.parquet\n├── age=71\n│   └── part-0.parquet\n├── age=72\n│   └── part-0.parquet\n├── age=73\n│   └── part-0.parquet\n├── age=74\n│   └── part-0.parquet\n├── age=75\n│   └── part-0.parquet\n├── age=76\n│   └── part-0.parquet\n├── age=77\n│   └── part-0.parquet\n├── age=78\n│   └── part-0.parquet\n├── age=79\n│   └── part-0.parquet\n├── age=80\n│   └── part-0.parquet\n├── age=81\n│   └── part-0.parquet\n├── age=82\n│   └── part-0.parquet\n├── age=83\n│   └── part-0.parquet\n├── age=84\n│   └── part-0.parquet\n├── age=85\n│   └── part-0.parquet\n├── age=86\n│   └── part-0.parquet\n├── age=87\n│   └── part-0.parquet\n├── age=88\n│   └── part-0.parquet\n├── age=89\n│   └── part-0.parquet\n├── age=90\n│   └── part-0.parquet\n├── age=91\n│   └── part-0.parquet\n├── age=92\n│   └── part-0.parquet\n├── age=96\n│   └── part-0.parquet\n└── age=97\n    └── part-0.parquet\n```\n:::\n:::\n\n\nAs we can see, a folder has been created for each value of age and the rows where the original data matched that condition are contained in parquet files within.\n\nThe dataset directory is nonetheless still more efficient than the original csv.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# parquet arrow data set\nfs::dir_info(\"data/arrow_dataset\", recurse = TRUE)$size %>% sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n140M\n```\n:::\n\n```{.r .cell-code}\n# original csv\nfs::file_size(\"data/synthpop_10000000.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNA\n```\n:::\n:::\n\n\nNow that we've got a partitioned directory of our data, let's go ahead and open it as an arrow dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrow_dir_dataset <- arrow::open_dataset(\"data/arrow_dataset\", format = \"parquet\")\n```\n:::\n\n\n## Summary Benchmarks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    df = query_1(data),\n    sqlite = query_1(tbl) %>%\n        collect(),\n    sqlite_idx = query_1(tbl_idx) %>%\n        collect(),\n    duckdb = query_1(tbl_duckdb) %>%\n        collect(),\n    arrow_tbl = query_1(arrow_tbl) %>%\n    collect(),\n    arrow_csv_dataset = query_1(arrow_dt_csv) %>%\n    collect(),\n    arrow_dir_dataset = query_1(arrow_dir_dataset) %>%\n        collect(),\n    check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 6\n  expression             min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>        <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df                 117.2ms  117.3ms      8.53  269.51MB     8.53\n2 sqlite             609.7ms  609.7ms      1.64    3.44MB     0   \n3 sqlite_idx          65.7ms   67.2ms     14.6     3.44MB     2.09\n4 duckdb              87.2ms   87.9ms     11.4     1.58MB     5.69\n5 arrow_tbl           34.5ms   35.2ms     28.1   281.53KB     5.11\n6 arrow_csv_dataset  346.7ms  366.9ms      2.73  267.62KB     0   \n7 arrow_dir_dataset  653.8ms  653.8ms      1.53  426.72KB     0   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    df = query_2(data),\n    sqlite = query_2(tbl) %>%\n        collect(),\n    sqlite_idx = query_2(tbl_idx) %>%\n        collect(),\n    duckdb = query_2(tbl_duckdb) %>%\n        collect(),\n    arrow_tbl = query_2(arrow_tbl) %>%\n    collect(),\n    arrow_csv_dataset = query_2(arrow_dt_csv) %>%\n    collect(),\n    arrow_dir_dataset = query_2(arrow_dir_dataset) %>%\n        collect(),\n    check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 6\n  expression             min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>        <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 df                   801ms    801ms      1.25     435MB    13.7 \n2 sqlite             669.2ms  669.2ms      1.49     530KB     1.49\n3 sqlite_idx         527.5ms  527.5ms      1.90     528KB     1.90\n4 duckdb              94.7ms   95.5ms     10.3      856KB    12.1 \n5 arrow_tbl           59.2ms   62.7ms     15.7      762KB     5.88\n6 arrow_csv_dataset  221.1ms    223ms      4.48     189KB     1.49\n7 arrow_dir_dataset  798.2ms  798.2ms      1.25     222KB     0   \n```\n:::\n:::\n\n\n::: callout-important\n### Overall Take Aways\n\n-   DuckDB can be a very efficient database format for complex queries involving large amounts of data due to it's OLAP nature owing to it's columnar-vectorised operation engine.\n\n-   Indexing can improve queries in SQLite and other OLTP type databases. However they are not flexible, take a lot of knowledge and experimentation, increase disk space and can also reduce performance on other queries or if mis-applied.\n\n-   The arrow package provide another option for loading or opening connections to files or directories of data and has deep integration with dplyr for performing queries.\n\n-   Partitioning can improve querying directories of data as arrow datasets. they are however inflexible and represent a single physical index applied on the whole dataset.\n\n-   Arrow tables allows loading large datasets in a more memory efficient way and support very fast querying.\n:::\n\n# Batch processing\n\nIn the previous sections we were focusing on a narrow set of operations, in particular the efficiency of accessing, filtering, selecting, ordering and aggregating subsets of data from data too large to fit into memory. But often we need to perform some processing on the whole dataset, as we saw in the example of populating our database in batches.\n\nOther times our analyses, for example fitting a model, might require the full dataset to produce a result which can be problematic even if we can just load our data in our memory as that may leaves us with little RAM to compute.\n\nAn option in this case would be to use algorithms that can compute on chunks or batches of the data. These algorithms are known as *external memory algorithms* (EMA), or *batch processing*.\n\nHere's a simple example of how we might write a batch processing algorithm to calculate the mean across multiple files, specifically the parquet files we just created in `data/arrow_dataset/arrow_dat`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch_mean <- function(file_name) {\n    dat <- arrow::read_parquet(file_name, col_select = \"income\")\n    \n    income_vct <- na.omit(dat[[\"income\"]])\n    \n    c(mean = mean(income_vct),\n      n = length(income_vct))\n}\n```\n:::\n\n\nIn this function we are given a file name. For each file, we load only the column we are interested (`income`) remove `NA`s and calculate the mean. To be able to aggregate the mean across all files, we also record `n`, the number of values used to calculate the mean.\n\nWe can then apply the function to a list of file names and aggregate the results in a tibble using the `purrr::map_dfr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_names <- fs::dir_ls(\"data/arrow_dataset\", \n                         recurse = TRUE,\n                         type = \"file\")\n\nmeans <- purrr::map_dfr(file_names, ~batch_mean(.x))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 79 × 2\n    mean      n\n   <dbl>  <dbl>\n 1  543.   5138\n 2  530.  17675\n 3  548.  20947\n 4  678.  21002\n 5  770.  17801\n 6 1109.  71922\n 7 1107.  89002\n 8 1275.  95972\n 9 1411.  92471\n10 1555. 115432\n# … with 69 more rows\n```\n:::\n:::\n\n\nNow that we've got our batched mean we can calculate a weighted mean an use the n column as the weight, which indeed gives us the same mean as we had calculated it on the whole dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted.mean(x = means$mean, w = means$n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1639.416\n```\n:::\n\n```{.r .cell-code}\nmean(data$income, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1639.416\n```\n:::\n:::\n\n\n## Specialised R packages\n\nThere are a number of R packages that provide EMA solutions for analysis bigger than memory data.\n\nFor example function `biglm` from package `biglm` allows for fitting a linear model in batches.\n\nIn the following example from the package documentation, an `lm` model is fitted initially to a small subset of the data with function `biglm`. The model is subsequently updated with additional chunks of using the `update`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\nff<-log(Volume)~log(Girth)+log(Height)\n\nchunk1<-trees[1:10,]\nchunk2<-trees[11:20,]\nchunk3<-trees[21:31,]\n\nlibrary(biglm)\na <- biglm(ff,chunk1)\na <- update(a,chunk2)\na <- update(a,chunk3)\n\ncoef(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)  log(Girth) log(Height) \n  -6.631617    1.982650    1.117123 \n```\n:::\n:::\n\n\nThe list of R packages available are numerous and their suitability varies according to the data and analysis you need to perform.\n\n-   [**bigmemory**](https://cran.r-project.org/web/packages/bigmemory/bigmemory.pdf): Manage Massive Matrices with Shared Memory and Memory-Mapped Files. The package uses memory mapping where RAM addresses are mapped to a file on disk. While innnevitably reducing performance, this extends the memory available for computation to memory on disk\n\n    -   A number of analytics packages build on `bigmemory` including:\n\n        -   [**`bigtabulate`**](https://cran.r-project.org/web/packages/bigtabulate/bigtabulate.pdf): Extend the bigmemory package with 'table', 'tapply', and 'split' support for 'big.matrix' objects.\n\n        -   [`bigalgebra`](https://cran.r-project.org/web/packages/bigalgebra/index.html): For matrix operation.\n\n        -   [**`biganalytics`**](https://cran.r-project.org/web/packages/biganalytics/biganalytics.pdf): Extend the 'bigmemory' package with various analytics, eg bigkmeans.\n\n        -   [**`bigFastlm`**](https://github.com/jaredhuling/bigFastlm): for (fast) linear models.\n\n        -   [**`biglasso`**](https://yaohuizeng.github.io/biglasso/index.html): extends lasso and elastic nets.\n\n        -   [**`GHap`**](https://cran.r-project.org/web/packages/GHap/index.html): Haplotype calling from phased SNP data.\n\n        -   [**`oem`**](https://jaredhuling.org/oem/): Penalized Regression for Big Tall Data.\n\n        -   [**`bigstep`**](https://cran.r-project.org/web/packages/bigstep/vignettes/bigstep.html): Uses the **bigmemory** framework to perform stepwise model selection, when the data cannot fit into RAM.\n\n-   [**`ff`**](https://cran.r-project.org/web/packages/ff/index.html) : The ff package provides data structures that are stored on disk in a binary format but behave (almost) as if they were in RAM by transparently mapping only a section (pagesize) in main memory. These data structures lend themselves to efficient chunking. Unlike bigmemory which on support numeric data types, `ff` supports all of R vector types including factors (which any character data is converted to for memory efficiency.\n\n    -   [`ffbase`](https://cran.microsoft.com/snapshot/2020-04-20/web/packages/ffbase/ffbase.pdf): extends the `ff` package with a number of methods for working with `ff` objects.\n\n    -   Package `biglm` also has methods for `ff` type objects so is not limited to fitting on numeric data.\n\nA good place to find up to date information on available packages is the [CRAN Task View on High-Performance and Parallel Computing with R](https://cran.r-project.org/web/views/HighPerformanceComputing.html), especially the section on **Large memory and out-of-memory data**.\n\n::: callout-note\nI should acknowledge that this brief section has been a summarisation of the [chapter on Efficient Memory](http://www.john-ros.com/Rcourse/memory.html) from BGUs Department of Industrial Engineering and Management \"R\" course by Jonathan D. Rosenblatt. For more information I highly recommend reviewing it as well as the chapter on [Sparse Representations](http://www.john-ros.com/Rcourse/sparse.html).\n:::\n",
    "supporting": [
      "05c_big-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}