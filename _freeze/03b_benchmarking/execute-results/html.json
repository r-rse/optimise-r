{
  "hash": "34b5d80c0fb15336eb8fb8274cfdf40a",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking\"\n---\n\n\nBenchmarking effectively refers to timing execution of our code, although aspects of memory utilisation can also be compared during benchmarking.\n\nThere are many ways to benchmark code in R and here we'll review some of the most useful ones.\n\n## Simple benchmarking\n\n### `system.time()`\n\nThe function takes an expression and returns the CPU time used to evaluate the expression. It is a primitive function and part of base R an a good starting point for timing R expressions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n    for(i in 1:100) mad(runif(1000))\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.007   0.001   0.007 \n```\n:::\n:::\n\n\nThe output prints a named vector of length 3.\n\nThe first two entries are the **total user** and **system CPU** times of *the current **R** process and any child processes* on which it has waited, and the third entry is the **'real' elapsed time** since the process was started.\n\nThe resolution of the times will be system-specific.\n\nExpressions across multiple lines can be run by enclosing them in curly braces (`{}`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time({\n    for(i in 1:100) {\n        v <- runif(1000)\n        mad(v)\n    }\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.007   0.000   0.007 \n```\n:::\n:::\n\n\n### `tictoc` ðŸ“¦\n\nPackage `tictoc` provides similar functionality to `system.time` with some additional useful features. To time code execution, you can wrap them between function calls `tic` and `toc`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\n\ntic()\n\nfor (i in 1:100) {\n    v <- runif(1000)\n    mad(v)\n}\n\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.007 sec elapsed\n```\n:::\n:::\n\n\nThe nice features of the `tictoc` package is that;\n\n-   Descriptive messages can be associated with each timing through `tic` argument `msg`.\n\n-   Timings can be nested.\n\n-   Timings can be logged through `toc` argument `log = TRUE` and accessed afterwards through `tic.log()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic(msg = \"MAD 100 iterations of 1000 random values\")\n\nfor (i in 1:100) {\n    v <- runif(1000)\n    mad(v)\n}\n\ntoc(log = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAD 100 iterations of 1000 random values: 0.008 sec elapsed\n```\n:::\n\n```{.r .cell-code}\ntic(msg = \"MAD 1000 iterations of 1000 random values\")\n\nfor (i in 1:1000) {\n    v <- runif(1000)\n    mad(v)\n}\n\ntoc(log = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAD 1000 iterations of 1000 random values: 0.065 sec elapsed\n```\n:::\n\n```{.r .cell-code}\ntic(msg = \"MAD 100 iterations of 10000 random values\")\n\nfor (i in 1:100) {\n    v <- runif(10000)\n    mad(v)\n}\n\ntoc(log = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAD 100 iterations of 10000 random values: 0.044 sec elapsed\n```\n:::\n\n```{.r .cell-code}\ntic.log()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"MAD 100 iterations of 1000 random values: 0.008 sec elapsed\"\n\n[[2]]\n[1] \"MAD 1000 iterations of 1000 random values: 0.065 sec elapsed\"\n\n[[3]]\n[1] \"MAD 100 iterations of 10000 random values: 0.044 sec elapsed\"\n```\n:::\n:::\n\n\n## Formal Benchmarking\n\n`system.time()` and `tictoc` are straightforward and simple to use for timing individual of code. However, there do have a few limitations:\n\n-   They only time execution time of the code once. As we've seen there's a lot of other things going on on your system which may affect execution time at any one time. So timings can vary if tested repeatedly.\n\n-   Comparing different expressions has to be performed rather manually, especially if we want to also check expressions being compared give the same result.\n\nThere are a number of packages in R that make benchmarking code, especially the comparison of different expressions much easier and robust. Here we will explore two of these, `microbenchmark` and `bench`.\n\n### `microbenchmark` ðŸ“¦\n\nThe `microbenchmark()` function in package `microbenchmark` serves as a more accurate replacement of `system.time()`. To achieved this, the sub-millisecond (supposedly nanosecond) accurate timing functions most modern operating systems provide are used. This allows us to compare expressions with much shorter execution times.\n\nSome nice package features:\n\n-   By default evaluates each expression multiple times, by default 100 but this number can be controlled through argument `times`.\n\n-   You can enforce checks on the results to ensure each expression tested returns the same result, with various levels of strictness through argument `check`.\n\n-   You can supply setup code that will be run by each iteration without contributing to the timing through argument `setup`.\n\n-   Note that the function is only meant for micro-benchmarking small pieces of source code and to compare their relative performance characteristics.\n\nSee the function documentation for more info.\n\nLet's go ahead and look at an example.\n\n#### Centering data in data.frame by column mean\n\nLet's say we are given the following code and asked to speed it up.\n\nIn this example, first a data frame is created that has 151 columns. One of the columns contains a character ID, and the other 150 columns contain numeric values.\n\nFor each numeric column, the code calculates the mean and subtracts it from the values in the column, so the data in each column is centered on the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrows <- 400000\ncols <- 150\ndata <- as.data.frame(x = matrix(rnorm(rows * cols, mean = 5), ncol = cols))\ndata <- cbind(id = paste0(\"g\", seq_len(rows)), data)\n\n# Get column means\nmeans <- apply(data[, names(data) != \"id\"], 2, mean)\n\n# Subtract mean from each column\nfor (i in seq_along(means)) {\n    data[, names(data) != \"id\"][, i] <- data[, names(data) != \"id\"][, i] - means[i]\n}\n```\n:::\n\n\nLooking at it, we might think back to the age old R advice to \"avoid loops\". So to improve performance of our code we might start by working on the for loop.\n\nLet's use `microbenchmark` to test the execution times of alternative approaches and compare them to the original approach:\n\n-   Give we we want to iterate across more than one objects (`means` and the columns in our data) might consider the `mapply` function. We then reassign the result back to the appropriate columns in our data.frame.\n-   We could also try `purrr`s `map2_dfc` which takes two inputs and column binds the results into a data.frame. We can again reassign the result back to the appropriate columns in our data.frame.\n\nTo test out our approaches we might want to make a smaller version of our data to work with. Let's create a smaller data frame with 40,000 rows and 50 columns and re-calculate our `means` vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrows <- 40000\ncols <- 50\ndata <- as.data.frame(x = matrix(rnorm(rows * cols, mean = 5), ncol = cols))\ndata <- cbind(id = paste0(\"g\", seq_len(rows)), data)\n\nmeans <- apply(data[, names(data) != \"id\"], 2, mean)\n```\n:::\n\n\nNow let's wrap our original for loop and our two test approaches in a benchmark.\n\nWe can wrap each expression in `{}` and pass it as a named argument for easier review of our benchmark results.\n\nLet's only run 50 iterations instead of the default 100 through argument `times`.\n\nLet's also include some `setup` code so that a new `data_bnch` object is created before each benchmark expression so that we don't overwrite the `data` object in the global environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(\n    for_loop = {\n        for (i in seq_along(means)) {\n            data_bnch[, names(data_bnch) != \"id\"][, i] <- \n                data_bnch[, names(data_bnch) != \"id\"][, i] - means[i]\n        }\n    }, \n    mapply = {\n        data_bnch[, names(data_bnch) != \"id\"] <- mapply(\n            FUN = function(x, y) x - y,\n            data_bnch[, names(data_bnch) != \"id\"], \n            means)\n    },\n      map2_dfc = {\n        data_bnch[, names(data_bnch) != \"id\"] <- purrr::map2_dfc(\n                            data_bnch[, names(data_bnch) != \"id\"], \n                            means, \n                            ~.x - .y)\n    },\n    times = 50,\n    setup = {data_bnch <- data}\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in microbenchmark::microbenchmark(for_loop = {: less accurate nanosecond\ntimes to avoid potential integer overflows\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n     expr       min        lq      mean   median        uq       max neval cld\n for_loop 11.322068 11.790288 15.891227 12.98288 14.449671  49.79237    50  a \n   mapply 38.921915 41.048503 53.442287 44.97823 72.746095  90.08610    50   b\n map2_dfc  3.248143  3.470773  9.176194  3.78020  6.166728 204.37163    50  a \n```\n:::\n:::\n\n\nThe results of our benchmark return one row per expression tested.\n\n-   `expr` contains the name of the expression.\n\n-   `min`, `lq`, `mean`, `median`, `uq` and `max` are summary statistics of the execution times across all iterations.\n\n-   `neval` shows the numbers of iterations\n\nSo far, `purrr::map2_dfc()` is looking like the best option.\n\nBut are we sure we are getting the same results form each approach?\n\nTo ensure this we can re-run our benchmarks using the `check` argument. A value of `\"equal\"` will compare all values output by the benchmark using `all.equal().` For the comparison to work, we need the last expression of the computation to output the same object. As this differs in the for loop from the two others, we include a call to print the final object `data_bnch` for comparison in each expression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(\n    for_loop = {\n        for (i in seq_along(means)) {\n            data_bnch[, names(data_bnch) != \"id\"][, i] <- \n                data_bnch[, names(data_bnch) != \"id\"][, i] - means[i]\n        }\n        data_bnch\n    }, \n    mapply = {\n        data_bnch[, names(data_bnch) != \"id\"] <- mapply(\n            function(x, y) x - y,\n            data_bnch[, names(data_bnch) != \"id\"], \n            means)\n        data_bnch\n    },\n      map2_dfc = {\n        data_bnch[names(data_bnch) != \"id\"] <- purrr::map2_dfc(\n                            data_bnch[, names(data_bnch) != \"id\"], \n                            means, \n                            ~.x - .y)\n        data_bnch\n    },\n    times = 50,\n    setup = {data_bnch <- data},\n    check = \"equal\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n     expr       min        lq      mean   median        uq       max neval cld\n for_loop 10.767543 11.455523 14.994899 11.57430 11.919971  78.84259    50  b \n   mapply 37.442717 39.848966 55.782725 42.43557 46.102573 111.46563    50   c\n map2_dfc  2.884965  3.325838  5.749091  3.67237  4.299342  71.68547    50 a  \n```\n:::\n:::\n\n\nExcellent! We can now be confident that our tests are retuning the same result.\n\nFinally, if you like to compare things visually, the output of our benchmark can be provided as an input to the `autoplot.microbenchmark` method to produce a graph of microbenchmark timings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nmicrobenchmark::microbenchmark(\n    for_loop = {\n        for (i in seq_along(means)) {\n            data_bnch[, names(data_bnch) != \"id\"][, i] <- \n                data_bnch[, names(data_bnch) != \"id\"][, i] - means[i]\n        }\n        data_bnch\n    }, \n    mapply = {\n        data_bnch[, names(data_bnch) != \"id\"] <- mapply(\n            function(x, y) x - y,\n            data_bnch[, names(data_bnch) != \"id\"], \n            means)\n        data_bnch\n    },\n      map2_dfc = {\n          data_bnch[names(data_bnch) != \"id\"] <- purrr::map2_dfc(\n                            data_bnch[, names(data_bnch) != \"id\"], \n                            means, \n                            ~.x - .y)\n        data_bnch\n    },\n    times = 50,\n    setup = {data_bnch <- data},\n    check = \"equal\"\n) %>%\nautoplot()\n```\n\n::: {.cell-output-display}\n![](03b_benchmarking_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### `bench` ðŸ“¦\n\n`bench` is similar to `microbenchmark`. However it offers some additional features which means I generally prefer it.\n\nThe main function equivalent to `microbenchmark()` is `mark()`.\n\n#### `mark()`\n\n**PROs**\n\n-   The main pro in my view is that it also tracks memory allocations for each expression.\n\n-   It also tracks the number and type of R garbage collections per expression iteration.\n\n-   It verifies equality of expression results by default, to avoid accidentally benchmarking inequivalent code.\n\n-   It allows you to execute code in separate environments (so that objects in global environment are not modified).\n\n**Some cons to consider:**\n\n-   it doesn't have a `setup` option.\n\n-   the output object, while much more informative than that of `microbenchmark` can be quite bloated itself.\n\nSo let's go ahead and run our tests using `bench::mark()`.\n\nBecause there is no setup option, we need to create `data_bnch` at the start of each expression. We can also use argument `env = new.env()` to perform all our computation in a separate environment.\n\nBecause `mark()` checks for equality by default, we use the version of our expressions that print the resulting `data_bnch` at the end for comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n    for_loop = {\n        data_bnch <- data\n        for (i in seq_along(means)) {\n            data_bnch[, names(data_bnch) != \"id\"][, i] <- \n                data_bnch[, names(data_bnch) != \"id\"][, i] - means[i]\n        }\n        data_bnch\n    }, \n    mapply = {\n        data_bnch <- data\n        data_bnch[, names(data_bnch) != \"id\"] <- mapply(\n            function(x, y) x - y,\n            data_bnch[, names(data_bnch) != \"id\"], \n            means)\n        data_bnch\n    },\n      map2_dfc = {\n          data_bnch <- data\n          data_bnch[names(data_bnch) != \"id\"] <- purrr::map2_dfc(\n                            data_bnch[, names(data_bnch) != \"id\"], \n                            means, \n                            ~.x - .y)\n        data_bnch\n    },\n    env = new.env()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 for_loop     9.09ms   9.75ms      91.0    17.4MB     21.8\n2 mapply      40.69ms  43.03ms      16.8   138.1MB     28.0\n3 map2_dfc      2.6ms   3.29ms     265.     15.3MB     39.8\n```\n:::\n:::\n\n\nLet's have a look at the output in detail:\n\n-   `expression` - `bench_expr` The deparsed expression that was evaluated (or its name if one was provided).\n\n-   `min` - The minimum execution time.\n\n-   `median` - The sample median of execution time.\n\n-   `itr/sec` - The estimated number of executions performed per second.\n\n-   `mem_alloc` - Total amount of memory allocated by R while running the expression.\n\n-   `gc/sec` - The number of garbage collections per second.\n\n-   `n_itr` - Total number of iterations after filtering garbage collections (if `filter_gc == TRUE`).\n\n-   `n_gc` - Total number of garbage collections performed over all iterations.\n\n-   `total_time` - The total time to perform the benchmarks.\n\n-   `result` - `list` A list column of the object(s) returned by the evaluated expression(s).\n\n-   `memory` - `list` A list column with results from [`Rprofmem()`](http://127.0.0.1:30837/help/library/bench/help/Rprofmem).\n\n-   `time` - `list` A list column of vectors for each evaluated expression.\n\n-   `gc` - `list` A list column with tibbles containing the level of garbage collection (0-2, columns) for each iteration (rows).\n\nI find the addition of the `mem_alloc` particularly useful. Just look at the difference between `mapply` and the other two approaches in term of memory usage!\n\nAs you can see, there's a lot more information in the `bench::mark()` output. Note as well there are a number of list columns at the which include results of the evaluated expressions, results of `Rprofmem()` and a list of garbage collection events. This can be quite useful to dig into.\n\nHowever, if this object is assigned to a variable or you try to save it, it could take up A LOT of space depending on the size of the results and number of internal calls. So I recommend getting rid of such columns if you want to save benchmarks.\n\n#### `press()`\n\nAnother cool feature of the `bench` package is bench::pressing() using the `press()` function. `press()` can be used to run [`mark()`](http://127.0.0.1:30837/help/library/bench/help/mark) across a grid of parameters and then *press* the results together.\n\nWe set the parameters we want to test across as named arguments and a grid of all possible combinations is automatically created.\n\nThe code to setup the benchmark is passed as a single unnamed expression before calling the bench::mark() code we want to run with the grid of parameters.\n\nLet's have a look at how this works.\n\nSay we want to test the performance of our three approaches on data.frames of different sizes varying both rows and columns.\n\nWe can specify two parameters in bench::press() as named arguments `rows` and `columns` and assigns vectors of the values we want press() to create a testing grid from.\n\nThe next curly braces `{}` contain our setup code which create data.frames of different sizes and the benchmark.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp <- bench::press(\n    rows = c(1000, 10000, 400000),\n    cols = c(10, 50, 100),\n    {\n        {\n            # Bench press setup code:\n            # create data.frames of different sizes using parameters \n            # rows & columns\n            set.seed(1)\n            data <- as.data.frame(x = matrix(\n                rnorm(rows * cols, mean = 5), \n                ncol = cols))\n            data <- cbind(id = paste0(\"g\", seq_len(rows)), data)\n            means <- apply(data[, names(data) != \"id\"], 2, mean)\n        }\n        bench::mark(\n            for_loop = {\n                data_bnch <- data\n                for (i in seq_along(means)) {\n                    data_bnch[, names(data_bnch) != \"id\"][, i] <- \n                        data_bnch[, names(data_bnch) != \"id\"][, i] - means[i]\n                }\n                data_bnch\n            }, \n            mapply = {\n                data_bnch <- data\n                data_bnch[, names(data_bnch) != \"id\"] <- mapply(\n                    function(x, y) x - y,\n                    data_bnch[, names(data_bnch) != \"id\"], \n                    means)\n                data_bnch\n            },\n            map2_dfc = {\n                data_bnch <- data\n                data_bnch[names(data_bnch) != \"id\"] <- purrr::map2_dfc(\n                    data_bnch[, names(data_bnch) != \"id\"], \n                    means, \n                    ~.x - .y)\n                data_bnch\n            },\n            env = new.env(),\n            time_unit = \"us\"\n        )\n    }\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRunning with:\n    rows  cols\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n1   1000    10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2  10000    10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n3 400000    10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n4   1000    50\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n5  10000    50\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n6 400000    50\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n7   1000   100\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n8  10000   100\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n9 400000   100\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n:::\n\n```{.r .cell-code}\nbp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 Ã— 8\n   expression   rows  cols    min  median `itr/sec` mem_alloc `gc/sec`\n   <bch:expr>  <dbl> <dbl>  <dbl>   <dbl>     <dbl> <bch:byt>    <dbl>\n 1 for_loop     1000    10   556.    583.   1694.     78.87KB    14.7 \n 2 mapply       1000    10   217.    259.   3884.    754.36KB    25.6 \n 3 map2_dfc     1000    10  1347.   1521.    636.     82.87KB     6.28\n 4 for_loop    10000    10   574.    632.   1592.    781.99KB    15.1 \n 5 mapply      10000    10  1540.   1932.    527.      7.11MB    34.6 \n 6 map2_dfc    10000    10  1376.   1569.    591.    785.99KB     6.24\n 7 for_loop   400000    10  1318.   2987.    165.     30.52MB    53.7 \n 8 mapply     400000    10 93752. 173903.      6.32  276.14MB    19.0 \n 9 map2_dfc   400000    10  2749.   4388.    138.     30.52MB    29.6 \n10 for_loop     1000    50  8397.   8619.    116.      2.52MB    23.2 \n# â€¦ with 17 more rows\n```\n:::\n:::\n\n\nNow when we look at our benchmark we see we get results for each approach and also for each `row` x `column` combination.\n\nLet's plot the results again using `autoplot` to get a better overview of our results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required namespace: tidyr\n```\n:::\n\n::: {.cell-output-display}\n![](03b_benchmarking_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nWhat thus show is that:\n\n-    for the smallest data.frame sizes, `mapply` is actually quite performant!\n\n-   for loops are also fastest when the number of columns is small regardless of number of rows.\n\n-   `map2_dfc` becomes the better performer as number of columns increases.\n",
    "supporting": [
      "03b_benchmarking_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}