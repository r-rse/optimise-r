---
title: "Best practice for writing efficient code"
---

We will look at a lot of examples here that will look as a strange way to do things. It's instructive often to go ahead and benchmark and memory profile such code to make what's going on more concrete.

## Avoid growing data structures

```{r}
x <- runif(10000, min = 1, max = 100)
```

Let's say we want to process our vector with a simple algorithm. If the value of `x` is less that 10, recode to 0. Otherwise, take the square root.

The most basic implementation might be to a `for` loop and process each element individually using an `if` statement and storing the output in another vector.

```{r}
#| eval: false
y <- NULL
for (i in seq_along(x)) {
    if (x[i] < 10) {
        y[i] <- 0
    } else {
        y[i] <- sqrt(x[i])
    }
}
y
```

Let's go ahead and make a starting benchmark.

```{r}
bench::mark(
    for_loop = {
        y <- NULL
        for (i in seq_along(x)) {
            if (x[i] < 10) {
                y[i] <- 0
            } else {
                y[i] <- sqrt(x[i])
            }
        }
        y   
    }
)
```

As we will see this is actually quite slow. What also stands out is the memory allocated to the execution, especially compared to the size of the original vector!

```{r}
lobstr::obj_size(x)
```

The biggest problem with this implementation, and the reason for loops are so often maligned for inefficiency is the fact that we are incrementally growing the result vector `y`. Because `y` is modified with every iteration, R has to make a copy of the original `y` vector before it can modify it and this takes up more and more memory as the vector grows.

Let's see what happens when we pre-allocate the results vector. And let's try three different versions:

-   a vector of `NA`s the same length as `x` (which defaults to a `logical` vector.

-   a vector of `NA_real_`s the same length as `x` . Using `NA_real_` creates a `double` vector.

-   a numeric vector the same length as `x` which creates a double vector of zeros.

```{r}
bench::mark(
    grow_res = {
        y <- NULL
        for (i in seq_along(x)) {
            if (x[i] < 10) {
                y <- c(y, 0)
            } else {
                y <- c(y, sqrt(x[i]))
            }
        }
        y
    },
    pre_all_NA = {
        y <- rep(NA, times = length(x))
        for (i in seq_along(x)) {
            if (x[i] < 10) {
                y[i] <- 0
            } else {
                y[i] <- sqrt(x[i])
            }
        }
        y
    },
    pre_all_NA_real = {
        y <- rep(NA_real_, times = length(x))
        for (i in seq_along(x)) {
            if (x[i] < 10) {
                y[i] <- 0
            } else {
                y[i] <- sqrt(x[i])
            }
        }
        y
    },
    pre_all_dat_type = {
        y <- numeric(length = length(x))
        for (i in seq_along(x)) {
            if (x[i] < 10) {
                y[i] <- 0
            } else {
                y[i] <- sqrt(x[i])
            }
        }
        y
    }
)


```

We can see a huge improvement in performance if we pre-allocate `y` to the appropriate and this is also reflected in the huge drop in memory allocation for the operation. Now R is making copies at every iteration but rather modifying `y` in place.

We also see minor speed up by preallocating the appropriate type of vector. This means R doesn't have to coerce the vector to the appropriate type in the first iteration.

## Vectorise where possible

```{r}

bench::mark(
    pre_all_dat_type = {
        y <- numeric(length = length(x))
        for (i in seq_along(x)) {
            y[i] <- sqrt(x[i])
        }
        y
    },
    unlist(lapply(x, sqrt)), 
    sapply(x, sqrt),
    purrr::map(x, sqrt)
)
```

```{r}

cond_sqrt_fl <- function(x) {
    y <- numeric(length = length(x))
        for (i in seq_along(x)) {
            if (x[i] < 10) {
                y[i] <- 0
            } else {
                y[i] <- sqrt(x[i])
            }
        }
        y
}

cond_sqrt_i <- function(x) {
    if (x < 10) {
        return(0)
    } else {
        return(sqrt(x))
    }
}

bench::mark(
    for_loop = cond_sqrt_fl(x),
    lapply = unlist(lapply(x, cond_sqrt_i)), 
    sapply = sapply(x, cond_sqrt_i),
    map_dbl = purrr::map_dbl(x, cond_sqrt_i)
)
```

Vectorising your code is not just about avoiding for loops, although that's often a step. Vectorising is about taking a whole-object approach to a problem, thinking about vectors, not scalars. There are two key attributes of a vectorised function:

-   It makes many problems simpler. Instead of having to think about the components of a vector, you only think about entire vectors.

-   The loops in a vectorised function are written in C instead of R. Loops in C are much faster because they have much less overhead.

```{r}
bench::mark(
    for_loop = cond_sqrt_fl(x),
    lapply = unlist(lapply(x, cond_sqrt_i)), 
    sapply = sapply(x, cond_sqrt_i),
    map_dbl = purrr::map_dbl(x, cond_sqrt_i),
    ifelse = ifelse(x < 10, 0, sqrt(x))
)
```

```{r}
cond_sqrt_vctr <- function(x) {
    x[x < 10] <- 0
    sqrt(x)
}
```

```{r}
bench::mark(
    for_loop = cond_sqrt_fl(x),
    lapply = unlist(lapply(x, cond_sqrt_i)), 
    sapply = sapply(x, cond_sqrt_i),
    map_dbl = purrr::map_dbl(x, cond_sqrt_i),
    ifelse = ifelse(x < 10, 0, sqrt(x)),
    for_loop_vct = cond_sqrt_vctr(x)
)
```

```{r}
bench::mark(
    rep(sqrt(x), length.out = length(x)),
    sqrt(x)
)
```

### Use floating point instead of double

```{r}
x_fl <- fl(x)

bench::mark(
    grow_res = {
        y <- NULL
        for (i in seq_along(x)) {
            y <- c(y, sqrt(x[i]))
        }
        y
    },
    pre_all_NA = {
        y <- rep(NA, times = length(x))
        for (i in seq_along(x)) {
            y[i] <- sqrt(x[i])
        }
        y
    },
    pre_all_dat_type = {
        y <- numeric(length = length(x))
        for (i in seq_along(x)) {
            y[i] <- sqrt(x[i])
        }
        y
    }
)

```

```{r}

```

## Use functions & operators implemented in C as much as possible

In base R, C code compiled into R at build time can be **called directly in what are termed *primitives*** *or via the* `.Internal` inteface.Primitives and Internals are implemented in C, they are generally fast.

Primitives are available to users to call directly. When writing code, a good rule of thumb as to get down to primitive functions with as few function calls as possible.

You can check whether a function is primitive with function `is.primitive()`

```{r}
is.primitive(switch)
is.primitive(abs)
is.primitive(ceiling)
is.primitive(`>`)

```

For more on this topic, check out the chapter on [internal vs primitive functions](https://cran.r-project.org/doc/manuals/r-release/R-ints.html#g_t_002eInternal-vs-_002ePrimitive) in the R Internals documentation.

### Detecting Internals

However while use of `.Internal()` to access underlying C functions is considered risky and for expert use only

(`?.Internal` includes the following in it's function description! ðŸ˜œ

> Only true **R** wizards should even consider using this function, and only **R** developers can add to the list of internal functions.

)

it's nonetheless good to be aware of which functions use .Internals and though implement

For example, when we check whether function `mean` is primitive, we get the `FALSE`.

```{r}
is.primitive(mean)
```

Digging a little deeper into the `mean.default` source code, however, we see that, after some input validation and handling, the function calls `.Internal(mean())` , i.e. the algorithm itself is executed using C.

```{r}
mean.default
```

To get a list of functions and operators implemented in C you can use `pryr::names_c()` .

```{r}
head(pryr::names_c())
```

This basically prints table `R_FunTab`, a table maintained by R of R function names and corresponding C functions to call (found in R source code in file [/src/main/names.c](https://github.com/wch/r-source/blob/trunk/src/main/names.c)).

Here's an interactive version you can use to search for functions by name to check whether they are implemented in C.

```{r}
#| echo: false
DT::datatable(pryr::names_c())
```

## Return early

## Streamline checks

## Memoise functions

```{r}
fib <- function(n) {
  if (n < 2) {
    return(n)
  } else {
    return(fib(n-1) + fib(n-2))
  }
}
```

```{r}
fib(30)

fibR(30)
```

## Use latest version of R

Each iteration of R is improving something to gain more speed with less and less memory. It's always useful if you need more speed to switch to Latest version of R and see if you get any speed gains.

For example, **as of R 3.4.0, R attempts to compile functions when they are first ran to byte code**. On subsequent function calls, instead of reinterpreting the body of the function, R executes the saved and compiled byte code. Typically, this results in faster execution times on later function calls.

Before this, if you wanted your functions byte compiled to improve performance, you needed to explicitly byte compile them using function `cmpfun` from package `compiler`.

In the following example, if you are running R \>= 3.4.0, you will see that the first time the user defined `csum` function is run takes longer than the second time because behind the scenes, R is byte compiling. You will also see that using `compile::cpmfun()` to byte compile the `csum` function afterwards has no effect as it already byte compiled.

If you are running an earlier version of R however you would see `compile::cpmfun()` having an effect on performance while the first two runs should take about the same time.

```{r}
csum <- function(x) {			
    if (length(x) < 2) return(x)			
    sum <- x[1]			
    for (i in seq(2, length(x))) {			
      sum[i] <- sum[i-1] + x[i]			
    }			
    sum			
 }	
 
tictoc::tic(msg = "First time function run (byte compiled by default in R >= 3.4.0)") 
res <- csum(data$V1)	
tictoc::toc() 

tictoc::tic(msg = "Second time function run (already byte compiled)") 
res <- csum(data$V1)
tictoc::toc() 

csum_cmp <- compiler::cmpfun(csum)

tictoc::tic(msg = "Explicit byte compilation does not improve performance") 
res <- csum_cmp(data$V1)
tictoc::toc() 
```

In general if you are using old versions of R or old functions that are deprecated and are no longer recommended by switching to a new version or new methods you will get a speed advantage for sure.

```{r}
tracemem(mtcars)

mtcars[mtcars$wt < 5, ]


mtcars %>% 
    filter(wt < 5) %>% 
    mutate(l100k = 235.21 / mpg) %>% # liters / 100 km
    group_by(cyl) %>% 
    summarise(l100k = mean(l100k))


```

```{r}
mtcars_tbl <- as_tibble(mtcars)

untracemem(mtcars)

tracemem(mtcars_tbl)
mtcars_tbl %>% 
    filter(wt < 5) %>% 
    mutate(l100k = 235.21 / mpg) 



mtcars_dt <- as.data.table(mtcars)
tracemem(mtcars_dt)
mtcars_dt[wt < 5]


mtcars_dt[wt < 5][, `:=`(l100k = 235.21/mpg)][, .(l100k = mean(l100k)), 
    keyby = .(cyl)]



mtcars_dtp <- lazy_dt(mtcars)
tracemem(mtcars_dtp)
mtcars_dtp %>% 
    filter(wt < 5) %>% 
    mutate(l100k = 235.21 / mpg) %>% # liters / 100 km
    group_by(cyl) %>% 
    summarise(l100k = mean(l100k)) %>%
    as_tibble()


```
